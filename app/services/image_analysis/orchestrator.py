import logging
import uuid
import json

import asyncpg
import boto3

from app.services.image_analysis import db_utils, openai_utils, s3_utils, pubsub_utils
from app.utils.config import Settings

settings = Settings()


async def process_image_analysis_job(
    slide_image_id: uuid.UUID, lecture_id: uuid.UUID, image_hash: str
):
    """
    Orchestrates the analysis of a single unique slide image.
    1. Fetches the image from S3.
    2. Sends it to an LLM for analysis (type, ocr_text, alt_text).
    3. Propagates the results to all DB records with the same image hash.
    4. Atomically increments the processed image counter for the lecture.
    5. If it's the last image, triggers the embedding job.
    """
    logging.info(
        f"Starting image analysis for slide_image_id={slide_image_id}, "
        f"lecture_id={lecture_id}, image_hash={image_hash}"
    )

    # Initialize clients
    if not settings.postgres_dsn:
        logging.error("Database settings are not configured.")
        raise RuntimeError("Required settings are not configured.")

    s3_client = boto3.client(
        "s3",
        aws_access_key_id=settings.s3_access_key or None,
        aws_secret_access_key=settings.s3_secret_key or None,
        endpoint_url=settings.s3_endpoint_url or None,
    )
    conn = None

    try:
        conn = await asyncpg.connect(settings.postgres_dsn)
        logging.info("Established connections to DB.")

        # 1. Verify lecture exists (Defensive Subscriber)
        if not await db_utils.verify_lecture_exists(conn, lecture_id):
            logging.warning(f"Lecture {lecture_id} not found. Stopping job.")
            return

        # 2. Get image path
        storage_path = await db_utils.get_image_storage_path(conn, slide_image_id)
        if not storage_path:
            logging.error(f"Storage path for slide_image {slide_image_id} not found.")
            return  # Acknowledge the message and stop.

        # 3. Download image from S3
        image_bytes = s3_utils.download_image(
            s3_client, settings.s3_bucket_name, storage_path
        )

        # 4. Analyze image with OpenAI
        if settings.mock_llm_calls:
            analysis_result = openai_utils.mock_analyze_image(image_bytes)
        else:
            analysis_result = await openai_utils.analyze_image(
                image_bytes=image_bytes,
                prompt="Analyze this image and return its type (content or decorative), any OCR text, and a descriptive alt text as a JSON object.",
            )

        # Use a transaction for the final updates to ensure atomicity
        async with conn.transaction():
            # 5. Propagate results to all matching images
            await db_utils.update_image_analysis_results(
                conn=conn,
                lecture_id=lecture_id,
                image_hash=image_hash,
                image_type=analysis_result.image_type,
                ocr_text=analysis_result.ocr_text,
                alt_text=analysis_result.alt_text,
            )

            # 6. Increment counter and check if last job
            processed_count, total_count = (
                await db_utils.increment_processed_images_count(conn, lecture_id)
            )

        # 7. Trigger embedding job if all images are processed
        if total_count > 0 and processed_count == total_count:
            logging.info(
                f"All {total_count} images for lecture {lecture_id} have been processed. "
                "Triggering embedding job."
            )
            pubsub_utils.publish_embedding_job(lecture_id)
        else:
            logging.info(
                f"Processed {processed_count}/{total_count} images for lecture {lecture_id}."
            )

    except Exception as e:
        logging.error(
            f"Image analysis job failed for slide_image_id {slide_image_id}: {e}",
            exc_info=True,
        )
        if conn:
            error_info = {
                "service": "image_analysis",
                "slide_image_id": str(slide_image_id),
                "error": str(e),
            }
            await conn.execute(
                "UPDATE lectures SET search_error_details = $1::jsonb, updated_at = NOW() WHERE id = $2",
                json.dumps(error_info),
                lecture_id,
            )
        # Re-raising ensures the message is not acknowledged and will be redelivered
        raise
    finally:
        if conn:
            await conn.close()
            logging.info("Postgres connection closed.")

    logging.info(
        f"Successfully finished image analysis for slide_image_id={slide_image_id}"
    )
